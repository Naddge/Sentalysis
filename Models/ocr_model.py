# -*- coding: utf-8 -*-
"""OCR_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mDf_YhUFTrNq-SYAF8B4376Lld3b8eK0
"""

!pip install --upgrade datasets

from transformers import (
    VisionEncoderDecoderModel,
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer,
    TrOCRProcessor
)
from torchvision import transforms
from PIL import Image
import torch

#ok
import kagglehub

# Download latest version
path = kagglehub.dataset_download("robikscube/textocr-text-extraction-from-images-dataset")

print("Path to dataset files:", path)

#ok
import json

dataset_path = "/kaggle/input/textocr-text-extraction-from-images-dataset"
train_json = f"{dataset_path}/TextOCR_0.1_train.json"

# Carica il file JSON
with open(train_json) as f:
    data = json.load(f)

# Crea un file JSONL con solo {image_path, text}
output_path = "train_data_trocr.jsonl"
with open(output_path, "w", encoding="utf-8") as f_out:
    for img_id, img in list(data["imgs"].items())[:700]:  # Puoi aumentare il limite
        filename = img['file_name'].replace('train/', '')
        img_path = f"{dataset_path}/train_val_images/train_images/{filename}"

        # Trova gli ID delle annotazioni
        annotation_ids = data["imgToAnns"].get(img_id, [])

        # Recupera le trascrizioni, ignorando bounding box
        texts = []
        for ann_id in annotation_ids:
            ann = data["anns"][str(ann_id)]
            text = ann["utf8_string"].strip()
            if text:  # Esclude stringhe vuote
                texts.append(text)

        if texts:
            full_text = " ".join(texts)
            entry = {"image_path": img_path, "text": full_text}
            f_out.write(json.dumps(entry, ensure_ascii=False) + "\n")

print("✅ File JSONL pronto per il fine-tuning con TrOCR.")
!head -n 5 train_data_trocr.jsonl

from datasets import load_dataset
import os

# 1. Verifica che il file esista
file_path = "/content/train_data_trocr.jsonl"
assert os.path.exists(file_path), f"File non trovato: {file_path}"

# 2. Carica il dataset correttamente
try:
    # Prova prima con questo metodo (più comune)
    dataset = load_dataset("json", data_files=file_path)["train"]
except Exception as e:
    print(f"Metodo standard fallito: {e}\nProvando alternativa...")
    # Metodo alternativo se il primo non funziona
    from datasets import Dataset
    dataset = Dataset.from_json(file_path)

# 3. Verifica il risultato
print(f"Dataset caricato con {len(dataset)} esempi")
print(dataset[0])  # Mostra il primo esempio

from transformers import TrOCRProcessor, VisionEncoderDecoderModel, Seq2SeqTrainingArguments, Seq2SeqTrainer
from PIL import Image
import torch

# 1. Inizializza il processore e il modello
processor = TrOCRProcessor.from_pretrained("microsoft/trocr-base-stage1")  # <-- AGGIUNTO
model = VisionEncoderDecoderModel.from_pretrained("microsoft/trocr-base-stage1")

# 2. Congela il decoder
for param in model.decoder.parameters():
    param.requires_grad = False

# 3. Configurazione del training
training_args = Seq2SeqTrainingArguments(
    output_dir="./trocr-encoder-only",
    per_device_train_batch_size=8,
    num_train_epochs=3,
    learning_rate=5e-5,
    fp16=torch.cuda.is_available(),
    logging_steps=100,
    save_steps=500,
    remove_unused_columns=False,
    eval_strategy="no",
)

# 4. Funzione di preprocessing corretta
def preprocess_function(examples):
    # Apre le immagini
    images = [Image.open(img_path).convert("RGB") for img_path in examples["image_path"]]

    # Processa immagini e testo
    pixel_values = processor(images, return_tensors="pt").pixel_values
    labels = processor.tokenizer(
        examples["text"],
        padding="max_length",
        truncation=True,
        max_length=64,
        return_tensors="pt"
    ).input_ids

    return {
        "pixel_values": pixel_values,
        "labels": labels
    }

# 5. Applica il preprocessing
dataset = dataset.map(
    preprocess_function,
    batched=True,
    batch_size=8,
    remove_columns=["image_path", "text"]  # Rimuove le colonne originali
)

# 6. Crea e avvia il trainer
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
)
trainer.save_model()
processor.save_pretrained("./trocr-encoder-only")

from transformers import TrOCRProcessor, VisionEncoderDecoderModel
from PIL import Image
import torch

# Carica processor e modello dal checkpoint fine-tuned
model_path = "./trocr-encoder-only"
processor = TrOCRProcessor.from_pretrained(model_path)
model = VisionEncoderDecoderModel.from_pretrained(model_path)
model.eval()

from google.colab import files
uploaded = files.upload()
filename = next(iter(uploaded))
image = Image.open(filename).convert("RGB")


# Preprocessing e inferenza
pixel_values = processor(images=image, return_tensors="pt").pixel_values
with torch.no_grad():
    generated_ids = model.generate(pixel_values)

# Decodifica del testo
text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
print("Riconosciuto:", text)